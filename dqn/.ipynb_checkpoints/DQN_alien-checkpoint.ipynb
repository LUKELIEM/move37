{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"dqn_play.py\", line 56, in <module>\r\n",
      "    time.sleep(delta)\r\n",
      "KeyboardInterrupt\r\n"
     ]
    }
   ],
   "source": [
    "!python dqn_play.py -m ../checkpoints/AlienNoFrameskip-v4-best.dat  --env \"AlienNoFrameskip-v4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=3136, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=18, bias=True)\n",
      "  )\n",
      ")\n",
      "709: done 1 games, mean reward 130.000, eps 0.99, speed 893.74 f/s\n",
      "1413: done 2 games, mean reward 170.000, eps 0.99, speed 995.32 f/s\n",
      "Best mean reward updated 130.000 -> 170.000, model saved\n",
      "1909: done 3 games, mean reward 156.667, eps 0.98, speed 975.94 f/s\n",
      "2519: done 4 games, mean reward 155.000, eps 0.97, speed 999.38 f/s\n",
      "3107: done 5 games, mean reward 146.000, eps 0.97, speed 997.41 f/s\n",
      "3867: done 6 games, mean reward 155.000, eps 0.96, speed 982.76 f/s\n",
      "4591: done 7 games, mean reward 161.429, eps 0.95, speed 996.66 f/s\n",
      "5302: done 8 games, mean reward 167.500, eps 0.95, speed 999.09 f/s\n",
      "5867: done 9 games, mean reward 165.556, eps 0.94, speed 994.62 f/s\n",
      "6649: done 10 games, mean reward 175.000, eps 0.93, speed 996.28 f/s\n",
      "Best mean reward updated 170.000 -> 175.000, model saved\n",
      "7130: done 11 games, mean reward 177.273, eps 0.93, speed 961.85 f/s\n",
      "Best mean reward updated 175.000 -> 177.273, model saved\n",
      "7938: done 12 games, mean reward 181.667, eps 0.92, speed 981.58 f/s\n",
      "Best mean reward updated 177.273 -> 181.667, model saved\n",
      "8581: done 13 games, mean reward 186.154, eps 0.91, speed 974.30 f/s\n",
      "Best mean reward updated 181.667 -> 186.154, model saved\n",
      "9284: done 14 games, mean reward 192.857, eps 0.91, speed 976.27 f/s\n",
      "Best mean reward updated 186.154 -> 192.857, model saved\n",
      "9894: done 15 games, mean reward 184.667, eps 0.90, speed 965.97 f/s\n",
      "10646: done 16 games, mean reward 186.250, eps 0.89, speed 181.34 f/s\n",
      "11231: done 17 games, mean reward 186.471, eps 0.89, speed 159.63 f/s\n",
      "11849: done 18 games, mean reward 187.778, eps 0.88, speed 160.24 f/s\n",
      "12342: done 19 games, mean reward 186.316, eps 0.88, speed 163.03 f/s\n",
      "13096: done 20 games, mean reward 213.500, eps 0.87, speed 162.11 f/s\n",
      "Best mean reward updated 192.857 -> 213.500, model saved\n",
      "13721: done 21 games, mean reward 215.238, eps 0.86, speed 161.72 f/s\n",
      "Best mean reward updated 213.500 -> 215.238, model saved\n",
      "14616: done 22 games, mean reward 219.545, eps 0.85, speed 160.22 f/s\n",
      "Best mean reward updated 215.238 -> 219.545, model saved\n",
      "15255: done 23 games, mean reward 218.261, eps 0.85, speed 162.21 f/s\n",
      "15948: done 24 games, mean reward 221.667, eps 0.84, speed 159.02 f/s\n",
      "Best mean reward updated 219.545 -> 221.667, model saved\n",
      "16655: done 25 games, mean reward 220.800, eps 0.83, speed 151.22 f/s\n",
      "17326: done 26 games, mean reward 221.923, eps 0.83, speed 161.27 f/s\n",
      "Best mean reward updated 221.667 -> 221.923, model saved\n",
      "18202: done 27 games, mean reward 222.963, eps 0.82, speed 157.27 f/s\n",
      "Best mean reward updated 221.923 -> 222.963, model saved\n",
      "18881: done 28 games, mean reward 226.786, eps 0.81, speed 158.77 f/s\n",
      "Best mean reward updated 222.963 -> 226.786, model saved\n",
      "19592: done 29 games, mean reward 223.793, eps 0.80, speed 154.58 f/s\n",
      "20211: done 30 games, mean reward 222.667, eps 0.80, speed 158.40 f/s\n",
      "20931: done 31 games, mean reward 223.226, eps 0.79, speed 159.45 f/s\n",
      "21457: done 32 games, mean reward 222.500, eps 0.79, speed 157.50 f/s\n",
      "22184: done 33 games, mean reward 224.242, eps 0.78, speed 161.52 f/s\n",
      "22738: done 34 games, mean reward 223.824, eps 0.77, speed 154.89 f/s\n",
      "23381: done 35 games, mean reward 224.571, eps 0.77, speed 137.90 f/s\n",
      "24035: done 36 games, mean reward 223.611, eps 0.76, speed 138.17 f/s\n",
      "24639: done 37 games, mean reward 222.973, eps 0.75, speed 137.54 f/s\n",
      "25168: done 38 games, mean reward 220.526, eps 0.75, speed 114.23 f/s\n",
      "25804: done 39 games, mean reward 220.769, eps 0.74, speed 124.89 f/s\n",
      "26545: done 40 games, mean reward 222.250, eps 0.73, speed 124.72 f/s\n",
      "27193: done 41 games, mean reward 224.878, eps 0.73, speed 118.11 f/s\n",
      "28247: done 42 games, mean reward 229.286, eps 0.72, speed 117.28 f/s\n",
      "Best mean reward updated 226.786 -> 229.286, model saved\n",
      "28785: done 43 games, mean reward 229.070, eps 0.71, speed 126.03 f/s\n",
      "29493: done 44 games, mean reward 227.955, eps 0.71, speed 120.23 f/s\n",
      "30272: done 45 games, mean reward 228.222, eps 0.70, speed 123.32 f/s\n",
      "30953: done 46 games, mean reward 227.391, eps 0.69, speed 126.61 f/s\n",
      "31516: done 47 games, mean reward 227.021, eps 0.68, speed 118.66 f/s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.clock()  # time the training\n",
    "\n",
    "!python dqn_basic.py --cuda --env \"AlienNoFrameskip-v4\"\n",
    "\n",
    "end = time.clock()\n",
    "print('\\nTraining time: {:.2f} min'.format((end-start)/60.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0., 0., 0., 0., 0., 0.]], requires_grad=True)\n",
      "Normal(loc: torch.Size([1, 6]), scale: torch.Size([1, 6]))\n",
      "tensor([[ 0.8996,  2.9661,  1.0525, -0.4046,  2.1881,  0.0815]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal\n",
    "\n",
    "mu = torch.ones(1, 6)\n",
    "log_std = nn.Parameter(torch.ones(1, 6) * 0)\n",
    "dist  = Normal(mu, log_std.exp().expand_as(mu))\n",
    "action = dist.sample()\n",
    "\n",
    "print(log_std)\n",
    "print(dist)\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
