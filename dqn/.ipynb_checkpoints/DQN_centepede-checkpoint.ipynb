{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"dqn_play.py\", line 56, in <module>\r\n",
      "    time.sleep(delta)\r\n",
      "KeyboardInterrupt\r\n"
     ]
    }
   ],
   "source": [
    "!python dqn_play.py -m ../checkpoints/CentipedeNoFrameskip-v4-best.dat --env \"CentipedeNoFrameskip-v4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=3136, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=18, bias=True)\n",
      "  )\n",
      ")\n",
      "457: done 1 games, mean reward 663.000, eps 1.00, speed 886.14 f/s\n",
      "1529: done 2 games, mean reward 1847.000, eps 0.98, speed 756.44 f/s\n",
      "Best mean reward updated 663.000 -> 1847.000, model saved\n",
      "2255: done 3 games, mean reward 1911.333, eps 0.98, speed 884.33 f/s\n",
      "Best mean reward updated 1847.000 -> 1911.333, model saved\n",
      "2781: done 4 games, mean reward 1544.500, eps 0.97, speed 990.29 f/s\n",
      "3390: done 5 games, mean reward 1716.800, eps 0.97, speed 944.70 f/s\n",
      "3997: done 6 games, mean reward 1752.000, eps 0.96, speed 941.81 f/s\n",
      "4558: done 7 games, mean reward 1582.286, eps 0.95, speed 821.54 f/s\n",
      "5117: done 8 games, mean reward 1573.375, eps 0.95, speed 815.67 f/s\n",
      "5978: done 9 games, mean reward 1678.444, eps 0.94, speed 1009.61 f/s\n",
      "6352: done 10 games, mean reward 1566.400, eps 0.94, speed 888.99 f/s\n",
      "7100: done 11 games, mean reward 1541.182, eps 0.93, speed 878.72 f/s\n",
      "7763: done 12 games, mean reward 1640.917, eps 0.92, speed 924.34 f/s\n",
      "8383: done 13 games, mean reward 1628.462, eps 0.92, speed 955.54 f/s\n",
      "9100: done 14 games, mean reward 1724.714, eps 0.91, speed 961.51 f/s\n",
      "9950: done 15 games, mean reward 1691.200, eps 0.90, speed 928.83 f/s\n",
      "10604: done 16 games, mean reward 1621.812, eps 0.89, speed 131.62 f/s\n",
      "11385: done 17 games, mean reward 1667.353, eps 0.89, speed 126.17 f/s\n",
      "12006: done 18 games, mean reward 1649.556, eps 0.88, speed 125.39 f/s\n",
      "12719: done 19 games, mean reward 1795.158, eps 0.87, speed 123.73 f/s\n",
      "13452: done 20 games, mean reward 1777.250, eps 0.87, speed 121.67 f/s\n",
      "14415: done 21 games, mean reward 1949.762, eps 0.86, speed 124.17 f/s\n",
      "Best mean reward updated 1911.333 -> 1949.762, model saved\n",
      "15751: done 22 games, mean reward 2218.318, eps 0.84, speed 120.71 f/s\n",
      "Best mean reward updated 1949.762 -> 2218.318, model saved\n",
      "16438: done 23 games, mean reward 2215.348, eps 0.84, speed 128.93 f/s\n",
      "17535: done 24 games, mean reward 2374.000, eps 0.82, speed 125.04 f/s\n",
      "Best mean reward updated 2218.318 -> 2374.000, model saved\n",
      "18333: done 25 games, mean reward 2363.920, eps 0.82, speed 122.20 f/s\n",
      "18924: done 26 games, mean reward 2335.846, eps 0.81, speed 123.51 f/s\n",
      "19639: done 27 games, mean reward 2292.444, eps 0.80, speed 122.54 f/s\n",
      "20133: done 28 games, mean reward 2258.357, eps 0.80, speed 127.61 f/s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.clock()  # time the training\n",
    "\n",
    "!python dqn_basic.py --cuda --env \"CentipedeNoFrameskip-v4\"\n",
    "\n",
    "end = time.clock()\n",
    "print('\\nTraining time: {:.2f} min'.format((end-start)/60.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0., 0., 0., 0., 0., 0.]], requires_grad=True)\n",
      "Normal(loc: torch.Size([1, 6]), scale: torch.Size([1, 6]))\n",
      "tensor([[ 0.8996,  2.9661,  1.0525, -0.4046,  2.1881,  0.0815]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal\n",
    "\n",
    "mu = torch.ones(1, 6)\n",
    "log_std = nn.Parameter(torch.ones(1, 6) * 0)\n",
    "dist  = Normal(mu, log_std.exp().expand_as(mu))\n",
    "action = dist.sample()\n",
    "\n",
    "print(log_std)\n",
    "print(dist)\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
